å…ˆfeaturizeç»„ä¸ªè‡³å°‘4090


1.pip install vllm

2.pip install "numpy<2"

3.# åœ¨ ç»ˆç«¯çª—å£ 1 è¿è¡Œï¼š
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --api-key "sk-featurize" \
  --gpu-memory-utilization 0.85 \
  --max-model-len 8192

4.# åœ¨ ç»ˆç«¯çª—å£ 2 è¿è¡Œï¼š
featurize port export 8000

5.è¿è¡Œåï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š
Local port 8000 has been exported to 12345 You can visit http://workspace.featurize.cn:12345 if it's a http server.æŠŠä¸Šé¢è¿™ä¸ªåœ°å€ http://workspace.featurize.cn:xxxxx å¤åˆ¶ä¸‹æ¥ã€‚è¿™å°±æ˜¯ä½ çš„å…¬ç½‘ API åœ°å€



ç„¶ååœ¨è‡ªå·±ç”µè„‘é“¾æ¥æµ‹è¯•ï¼š

æ‰“å¼€anacondaï¼Œå¹¶ä¸‹è½½å¼€æºé¡¹ç›®https://github.com/AkagawaTsurunaki/ZerolanLiveRobot?tab=readme-ov-fileï¼Œå¦‚æœæƒ³è¦åªæ˜¯åšä»¥ä¸‹ç›´æ’­èŠå¤©æœºå™¨äººçš„è¯å…ˆä¸‹è¿™ä¸ªZerolanLiveRobotå°±å¥½äº†


1.curl http://workspace.featurize.cn:43189/v1/models -H "Authorization: Bearer sk-featurize"å¦‚æœæˆåŠŸï¼š ä½ ä¼šçœ‹åˆ°ä¸€å¤§ä¸²åŒ…å« Qwen/Qwen2.5-7B-Instruct çš„æ–‡å­—ã€‚å¦‚æœå¤±è´¥ï¼š è¯´æ˜è¿æ¥æœ‰é—®é¢˜ã€‚

2.å¦‚æœä¸Šé¢é‚£ä¸€æ­¥æˆåŠŸäº†ï¼Œè¯·ç”¨ä¸‹é¢è¿™ä¸ªè¶…é•¿çš„ä¸€è¡Œå‘½ä»¤æ¥æµ‹è¯•å¯¹è¯ã€‚
curl http://workspace.featurize.cn:43189/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer sk-featurize" -d "{\"model\": \"Qwen/Qwen2.5-7B-Instruct\", \"messages\": [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}]}"

3.è¿›å…¥ä½ çš„é¡¹ç›®æ–‡ä»¶å¤¹ï¼šcd /d D:\1111jin\neureo\ZerolanLiveRobot-main\ZerolanLiveRobot-main

4.conda create --name ZerolanLiveRobot python=3.11 -y

5.æ¿€æ´»ç¯å¢ƒï¼šconda activate ZerolanLiveRobot

6.å®‰è£…ä¾èµ–pip install -r requirements.txt

7.pip install "pydantic<=2.11"

8.å…ˆè¿è¡Œä¸€æ¬¡python main.pyä¸ºäº†ç”Ÿæˆé…ç½®æ–‡ä»¶




ç›®å‰åªæ­å»ºäº†äº‘ç«¯çš„å¤§è„‘ (LLM)ï¼Œä½†æœ¬åœ°çš„ASRã€TTSå’ŒOCR/ShowUIå¯¹åº”çš„æœåŠ¡ (ZerolanCore) å¹¶æ²¡æœ‰è¿è¡Œã€‚
å¦‚æœç°åœ¨ç›´æ¥å¯åŠ¨ï¼Œæœºå™¨äººä¼šå› ä¸ºè¿ä¸ä¸Šæœ¬åœ°çš„ 127.0.0.1:11000 è€Œç›´æ¥æŠ¥é”™å´©æºƒã€‚
ä¸ºäº†è®©æœºå™¨äººç«‹åˆ»èƒ½åŠ¨èµ·æ¥ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå…³æ‰æ‰€æœ‰æœ¬åœ°æ²¡è·‘èµ·æ¥çš„æœåŠ¡ï¼Œåªå¼€å¯äº‘ç«¯å¤§æ¨¡å‹ã€‚ç”¨è®°äº‹æœ¬æˆ– VS Code æ‰“å¼€ config.yaml


1.æ‰¾åˆ° llm: å¼€å¤´çš„éƒ¨åˆ†ï¼Œå°†æ•´æ®µå†…å®¹ä¿®æ”¹ä¸ºä¸‹é¢è¿™æ ·ï¼ˆ
# Configuration for the Large Language Model pipeline.
  llm:
    enable: True
    # å¡«å…¥ API Key
    api_key: 'sk-featurize'
    # ã€å…³é”®ã€‘å¼€å¯ OpenAI å…¼å®¹æ¨¡å¼ï¼ˆå› ä¸º vLLM æ˜¯å…¼å®¹ OpenAI åè®®çš„ï¼‰
    openai_format: True
    # æ¨¡å‹åç§°ï¼Œå¿…é¡»å’Œä½  Featurize å¯åŠ¨å‘½ä»¤é‡Œçš„å®Œå…¨ä¸€è‡´
    model_id: 'Qwen/Qwen2.5-7B-Instruct'
    # å¡«å…¥ Featurize çš„å…¬ç½‘åœ°å€ (æ³¨æ„åé¢è¦åŠ  /v1)
    # âš ï¸è¯·æ£€æŸ¥ä½ çš„ç«¯å£å·æ˜¯ä¸æ˜¯ 43189ï¼Œå¦‚æœ Featurize é‡å¯è¿‡ï¼Œç«¯å£å¯èƒ½ä¼šå˜ï¼
    predict_url: 'http://workspace.featurize.cn:43189/v1'
    # æµå¼åœ°å€å¡«ä¸€æ ·çš„å³å¯
    stream_predict_url: 'http://workspace.featurize.cn:43189/v1'


å› ä¸ºæœ¬åœ°æ²¡æœ‰è¿è¡Œ ZerolanCoreï¼Œæˆ‘ä»¬éœ€è¦æŠŠç”¨åˆ°æœ¬åœ°æœåŠ¡çš„æ¨¡å—ï¼ˆé™¤äº†llmï¼‰å…¨éƒ¨è®¾ä¸º Falseã€‚

è¯·å‘ä¸‹æ»šåŠ¨ï¼Œæ‰¾åˆ°ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼ŒæŠŠå®ƒä»¬çš„ enable: True å…¨éƒ¨æ”¹ä¸º enable: Falseï¼š

asr (è¯­éŸ³è¯†åˆ«) -> enable: False

img_cap (çœ‹å›¾è¯´è¯) -> enable: False

ocr (æ–‡å­—è¯†åˆ«) -> enable: False

vid_cap (è§†é¢‘ç†è§£) -> enable: False

tts (è¯­éŸ³åˆæˆ) -> enable: False 

vla (è§†è§‰åŠ¨ä½œ) -> enable: False

vec_db (å‘é‡æ•°æ®åº“) -> enable: False

åœ¨æ–‡ä»¶çš„ååŠéƒ¨åˆ†æ‰¾åˆ° character:
character:
  # èµ·ä¸ªåå­—ï¼Œæ¯”å¦‚ "Qwen"
  bot_name: 'Qwen'







ç¨‹åºdebugï¼šç”±äºé‡‡ç”¨åƒé—®æ¨¡å‹å› æ­¤éœ€è¦æ·»åŠ ç›¸å…³è¯†åˆ«ã€‚

1.pipeline/llm/config.pyé‡Œé¢æ›¿æ¢ä¸º


from pydantic import Field

from common.enumerator import BaseEnum
from common.utils.enum_util import enum_to_markdown
from pipeline.base.base_sync import AbstractPipelineConfig


#######
# LLM #
#######

class LLMModelIdEnum(BaseEnum):
    DeepSeekAPI: str = "deepseek-chat"
    KimiAPI: str = "moonshot-v1-8k"

    ChatGLM3_6B: str = "THUDM/chatglm3-6b"
    GLM4: str = "THUDM/GLM-4"
    Qwen_7B_Chat: str = "Qwen/Qwen-7B-Chat"
    Qwen_2_5_7B_Instruct = 'Qwen/Qwen2.5-7B-Instruct'
    Shisa_7b_V1: str = "augmxnt/shisa-7b-v1"
    Yi_6B_Chat: str = "01-ai/Yi-6B-Chat"
    DeepSeek_R1_Distill: str = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"


class LLMPipelineConfig(AbstractPipelineConfig):
    api_key: str | None = Field(default=None, description="The API key for accessing the LLM service.ã€€\n"
                                                          "Kimi API supported: \n"
                                                          "Reference: https://platform.moonshot.cn/docs/guide/start-using-kimi-api \n"
                                                          "Deepseek API supported: \n"
                                                          "Reference: https://api-docs.deepseek.com/zh-cn/")
    openai_format: bool = Field(default=False, description="Whether the output format is compatible with OpenAI. \n"
                                                           f"Note: When you use `{LLMModelIdEnum.DeepSeekAPI}` or {LLMModelIdEnum.KimiAPI}, please set it `true`.")
    model_id: LLMModelIdEnum = Field(default=LLMModelIdEnum.GLM4,
                                     description=f"The ID of the model used for LLM. \n{enum_to_markdown(LLMModelIdEnum)}")
    predict_url: str = Field(default="http://127.0.0.1:11000/llm/predict",
                             description="The URL for LLM prediction requests.")
    stream_predict_url: str = Field(default="http://127.0.0.1:11000/llm/stream-predict",
                                    description="The URL for streaming LLM prediction requests.")






2.åŒä¸Špipeline/llmæ–‡ä»¶å¤¹çš„llm_sync.pyæ›¿æ¢ï¼š


from typing import Any
from pipeline.base.base_sync import CommonModelPipeline
from pipeline.llm.config import LLMPipelineConfig

class LLMSyncPipeline(CommonModelPipeline):

    def __init__(self, config: LLMPipelineConfig):
        super().__init__(config)
        self.api_key = getattr(config, 'api_key', 'EMPTY')
        self.predict_url = getattr(config, 'predict_url', '')
        
        # 1. å°è¯•è·å–æ¨¡å‹åå­—çš„å­—ç¬¦ä¸²å€¼ï¼Œé˜²æ­¢ Enum æŠ¥é”™
        raw_id = getattr(config, 'model_id', '')
        if hasattr(raw_id, 'value'):
            self.model_id = raw_id.value
        else:
            self.model_id = str(raw_id)
            
        self.openai_format = getattr(config, 'openai_format', True)

    def predict(self, query) -> Any:
        """
        å…¨èƒ½é¢„æµ‹å‡½æ•°
        """
        # è·å–è¾“å…¥å†…å®¹
        user_text = getattr(query, 'text', str(query))
        history = getattr(query, 'history', [])

        # 2. ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶ä¿®æ­£æ¨¡å‹å
        # å¦‚æœé…ç½®é‡Œçš„åå­—çœ‹èµ·æ¥åƒ Qwenï¼Œå°±å¼ºåˆ¶ç”¨ Featurize è®¤è¯†çš„åå­—
        real_model_name = self.model_id
        if "Qwen" in str(real_model_name):
            real_model_name = "Qwen/Qwen2.5-7B-Instruct"

        print(f"ğŸ¤– [AIæ€è€ƒä¸­] æ¨¡å‹: {real_model_name} | å†…å®¹: {user_text}")

        # æ£€æŸ¥ openai åº“
        try:
            from openai import OpenAI
        except ImportError:
            print("âŒ é”™è¯¯: ç¼ºå°‘ openai åº“ï¼Œè¯·è¿è¡Œ pip install openai")
            return {
                "response": "ç¼ºå°‘ openai ä¾èµ–åº“ã€‚",
                "history": history
            }

        try:
            # 3. ä¿®æ­£ URL (è¡¥å…¨ /v1)
            base_url = self.predict_url
            if "featurize" in base_url and not base_url.endswith("/v1"):
                base_url = base_url.rstrip("/") + "/v1"
            
            # è¿æ¥äº‘ç«¯
            client = OpenAI(api_key=self.api_key, base_url=base_url)

            # æ„å»ºå¯¹è¯
            messages = []
            if history:
                messages.append({"role": "system", "content": history[0].content})
                for h in history[1:]:
                    messages.append({"role": h.role, "content": h.content})
            messages.append({"role": "user", "content": user_text})

            # å‘é€è¯·æ±‚
            response = client.chat.completions.create(
                model=real_model_name, # è¿™é‡Œç”¨äº†ä¿®æ­£åçš„åå­—
                messages=messages,
                max_tokens=512,
                temperature=0.7,
                extra_body={"stop_token_ids": [151643]} # Qwen é˜²åºŸè¯
            )

            # è·å–å›å¤
            result_text = response.choices[0].message.content
            print(f"âœ… [AIå›å¤] {result_text}")
            
            # å°è¯•æ›´æ–°å†å²è®°å½• (è®©æœºå™¨äººè®°ä½åˆšæ‰è¯´çš„è¯)
            try:
                from zerolan.data.pipeline.llm import RoleEnum, Conversation
                # è¿™ä¸€æ­¥å¯èƒ½ä¼šå¤±è´¥å¦‚æœåŒ…æ²¡è£…å¥½ï¼Œæ‰€ä»¥åŠ äº† try
                # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¸»è¦ä¾èµ–è¿”å›çš„å­—å…¸
                if hasattr(query, 'history'):
                    query.history.append(Conversation(role=RoleEnum.user, content=user_text))
                    query.history.append(Conversation(role=RoleEnum.assistant, content=result_text))
            except:
                pass # å¦‚æœæ›´æ–°å†å²å¤±è´¥å°±ç®—äº†ï¼Œä¸å½±å“å›å¤

            # 4. ã€å…³é”®ä¿®å¤ã€‘è¿”å›å­—å…¸æ ¼å¼ (Dict)
            # Pydantic çœ‹åˆ°å­—å…¸ä¼šè‡ªåŠ¨æŠŠå®ƒè½¬æ¢æˆå®ƒæƒ³è¦çš„å¯¹è±¡ï¼Œä¸ä¼šå†æŠ¥é”™äº†
            return {
                "response": result_text,
                "history": history
            }

        except Exception as e:
            print(f"âŒ [è°ƒç”¨å¤±è´¥] {e}")
            # å‡ºé”™ä¹Ÿè¿”å›å­—å…¸
            return {
                "response": f"å¤§è„‘è¿æ¥æ–­å¼€äº†: {e}",
                "history": history
            }

    def stream_predict(self, query, chunk_size=None):
        return self.predict(query)






###################################è¿è¡Œ###############################################

æ‰“å¼€anaconda

1.cd /d D:\1111jin\neureo\ZerolanLiveRobot-main\ZerolanLiveRobot-main

2.æ¿€æ´»ç¯å¢ƒï¼šconda activate ZerolanLiveRobot

3.è¿è¡Œ python main.py

4.è¿è¡ŒæˆåŠŸåæ‰¾å‘½ä»¤é‡Œçš„é…ç½®ç½‘å€* Running on local URL:  http://127.0.0.1:7860ã€‚åœ¨é‡Œé¢é…ç½®ç›´æ’­æµï¼Œè¿›å…¥service -> live_stream -> bilibiliï¼Œå‹¾é€‰enableï¼Œç„¶åè¾“å…¥æˆ¿é—´å·ä»¥åŠä¸‹é¢çš„sessdataå•¥çš„ï¼ˆé…ç½®ç½‘é¡µæœ‰è·å–æ•™ç¨‹ï¼Œå°±æ˜¯edgef12ç‚¹å·¥å…·æ ä¸Šé¢çš„åº”ç”¨ç¨‹åºç„¶åæ‰©å±•å¼€å·¦è¾¹æ çš„cookiesï¼Œæ‰¾åˆ°live.bilibiliå°±è¡Œï¼‰
ä½ çœ‹åˆ°çš„æ—¥å¿—ï¼š
Config file was saved: D:\...\resources\config.yaml Function ... on_config_modified execution costs 0.0 s
è¿™è¯´æ˜ä½ é€šè¿‡ç½‘é¡µç«¯ (WebUI) ç‚¹å‡»â€œSave Configâ€åï¼Œæœºå™¨äººå·²ç»æŠŠæ–°å‚æ•°å†™å…¥äº†ç¡¬ç›˜ã€‚

5.å¯¹äº†ï¼Œæ”¹å®Œé‡å¯

